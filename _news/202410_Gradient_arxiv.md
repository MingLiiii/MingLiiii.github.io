---
layout: post
date: 2024-10-31
inline: true
---

One paper was put on the arXiv: [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743), where we try to understand the layer-wise gradient behaviors when LLMs are finetuned on Fast vs. Slow Thinking. Repo: [Layer_Gradient](https://github.com/MingLiiii/Layer_Gradient).
